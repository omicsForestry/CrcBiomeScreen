% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/EvaluateCrcBiomeScreen.R
\name{EvaluateCrcBiomeScreen}
\alias{EvaluateCrcBiomeScreen}
\title{Evaluate the performance of model predictions}
\usage{
EvaluateCrcBiomeScreen(
  predictions,
  true_labels,
  TrueLabel = NULL,
  TaskName = "ModelEvaluation",
  PlotAUC = TRUE
)
}
\arguments{
\item{predictions}{A data frame or matrix of model predictions, typically
containing columns for probability scores for each class.}

\item{true_labels}{A character vector or factor of the true class labels.}

\item{TrueLabel}{The positive class label (e.g., "CRC") to use for ROC/AUC calculation.}

\item{TaskName}{A character string used to label the output files.}

\item{PlotAUC}{A logical value indicating whether to plot the AUC curve.}
}
\value{
A list containing the ROC curve object and the AUC value.
}
\description{
This function calculates performance metrics (e.g., AUC) and plots the
ROC curve based on prediction probabilities and true labels.
}
\examples{
# Assume you have obtained predictions and true labels from a previous step
# e.g., predictions <- PredictCrcBiomeScreen(my_object, newdata, "RF")
# true_labels <- my_newdata_object$SampleData$study_condition
# EvaluateCrcBiomeScreen(predictions, true_labels, TrueLabel = "CRC", TaskName = "MyEvaluation", PlotAUC = TRUE)
}
